进程：cpu最小分配单元，每个运行中的程序至少包含一个进程，不同进程使用不同内存空间


多进程方式一(fork函数, 适用于Unix/GNU Linux/Mac):
fork()函数返回两次. 在父进程中返回子进程的ID, 在子进程中返回0
例.
import os

print('Process %s start.'%os.getpid())
pid = os.fork()
if pid == 0:
	print('I am child process %s, i inherit from process %s.'%(os.getpid(), os.getppid()))

结果
Process 6535 start.
I am parent process 6535, i have a child process 6536.
I am child process 6536, i inherit from process 6535.


多进程方式二(multiprocessing.Process类和multiprocessing.pool.Pool类, 适用于Unix/GNU Linux/Mac/Windows系统):
例1.
from multiprocessing import Process
import os

def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')

例2.
子进程跟随父进程结束(子进程为daemonic进程)
import multiprocessing, time, os

def f(proc_name):
	print("%s的进程ID: %s." % (proc_name, os.getpid()))
	print("%s的父进程ID: %s." % (proc_name, os.getppid()))
	time.sleep(20)
	print("进程%s is done." % os.getpid())

if __name__ == "__main__":
	print("%s的进程ID: %s." % (multiprocessing.current_process().name, os.getpid()))
	process = multiprocessing.Process(target=f, args=("sub_proc",), daemon=True)
	process.start()
	time.sleep(5)
	print("进程%s is done." % os.getpid())


三种启动子进程的方式:
1.spawn - 只从父进程继承必要的资源. 速度比fork/forkserver慢得多. 支持Unix/Mac/Windows, 在Mac和Windows为默认选项

2.fork - 完整继承父进程资源. 无法使用该方法启动多线程的进程. 支持Unix/Mac, 在Unix中为默认选项

3.forkserver - 启动一个server process, 每当需要启动新的process时, 父进程连接到server procee, server process会fork一个新进程. server process为单线程进程. 只在可用pipe传递文件描述符的Unix系统中支持


multiprocessing模块
function:
cpu_count()
	当前系统cpu数量

current_process()
	返回当前进程对象

parent_process()
	返回当前进程的父进程对象

Pipe(duplex=True)
	创建一个管道，返回(conn1, conn2)， 即一对Connection对象
	duplex - 当为True，管道为双工管道；当为False时，单工管道，conn1只能接收信息，conn2只能发送信息

class
Process
variabl:
daemon
	设置或获取进程是否为daemon进程
	进程的初始daemon继承于父进程
	进程结束时，试图中断所有daemonic子进程
	daemonic进程不允许创建子进程，当其daemonic的父进程结束时，它跟着结束，并将其子进程遗留为孤儿进程
	daemonic进程不同于Unix daemon程序概念

exitcode
	子进程的exit code

name
	赋值或获取进程名

pid
	进程id

method:
__init__(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)
	创建一个子进程
	group - 保留, 主要与threading.Thread格式兼容
	target - 可调用的function, 被子进程的run()方法调用的函数
	name - str类型，进程的名称，默认为'Process-<N>'
	args - tuple类型，传入target方法的位置参数
	kwargs - dict类型，传入target方法的关键字参数
	daemon - bool类型，设置进程是否为守护线程。默认继承自父进程

close()
	关闭进程

is_alive()
	进程是否正在运行，返回bool对象

join(timeout=None)
	子进程阻塞父进程，直到子进程结束
	timeout - 子进程阻塞指定时间，后续分离执行

kill()
	SIGKILL signal中断

start()
	启动子进程，实质上调用run()

terminate()
	SIGTERM signal中断


例3.
所有进程在进程池中处于同一个队列, sync进程会阻塞不让下一个进程执行, async不会如此
import time, os
from multiprocessing.pool import Pool

def do_sync(num):
    print("sync process %d start, process id: %s."%(num, os.getpid()))
    time.sleep(10)

def do_async(num):
    print("async process %d start, process id: %s."%(num, os.getpid()))
    time.sleep(15)

p = Pool(4)
start = time.time()
p.apply_async(do_async, args=(1,))
p.apply(do_sync, args=(2,))
p.apply_async(do_async, args=(3,))
p.apply(do_sync, args=(4,))
p.close()
p.join()
end = time.time()
print("cost time %.2f"%(end-start))
print("maip process is done.")


例4.
Pool类的map不会在进程在进行阻塞
import time, os, random
from multiprocessing.pool import Pool

def pow_num(x):
    pro = x*x
    print(pro)
    r = random.randint(10, 20)
    time.sleep(r)
    print('process id: %s, execute %d s.'%(os.getpid(), r))

p = Pool(4)
start = time.time()
l = range(1, 11)
p.map(pow_num, l, chunksize=2)
p.close()
p.join()
end = time.time()
print("cost time %.2f"%(end-start))
print("maip process is done.")


multiprocessing.pool模块
class
Pool
method:
__init__(processes=None, initializer=None, initargs=(), maxtasksperchild=None, context=None)
	创建进程池. 参数列表如下:
		processes - 进程池中的进程数量. 默认为os.cpu_count()返回的值
		initializer - 当不为None时, 进程池中的每个进程都调用该可调用函数
		initargs - 可调用函数的位置参数列表
		maxtasksperchild - 进程执行指定任务数量后关闭. 默认为存活直到线程池关闭

apply(func, args=(), kwds={})
	启动sync进程, 需要进程池中的上一个运行进程完毕, 再执行下一个进程. 参数列表如下:
	func - 调用的函数
	args - tuple格式，传递给调用函数的位置参数
	kwds - dict格式，传递给调用函数的关键字参数

apply_async(func, args=(), kwds={}, callback=None, error_callback=None)
	启动async进程, 进程池中的进程可随时切换执行
	func - 调用的函数
	args - tuple格式，传递给调用函数的位置参数
	kwds - dict格式，传递给调用函数的关键字参数

close()
	阻止新进程加入进程池

imap(func, iterable, chunksize=1)
	慵懒版本的map, 可用于处理大型iterables

join()
	进程池进程阻塞主进程

map(func, iterable, chunksize=1)
	将iterable中的元素逐个执行func的操作. 类似于built-in指令map. 块内保持同步阻塞(非进程间, 而是进程内的块单元操作). 参数列表如下:
		func - 可迭代对象中元素执行的操作
		iterable - 可迭代对象
		chunksize - 将可迭代对象切分的大小, 该切分后的块分配给不同process

map_async(func, iterable, chunksize=1, callback=None, error_callback=None)
	类似于map

terminate()
	马上中断进程池中所有进程


进程间通信方式一(multiprocessing.Queue类和multiprocessing.Pipe函数):
例1.
只能使用Process向Queue中添加元素, 无法使用Pool
queue是进程/线程安全的
from multiprocessing import Queue, Process

def put_in(q, *args):
    q.put(args)

if __name__ == '__main__':
    queue = Queue()
    p1 = Process(target=put_in, args=(queue, [1,2,3]))
    p2 = Process(target=put_in, args=(queue, 'i love you'))
    p1.start()
    p2.start()
    p1.join()
    p2.join()
    print(queue.get())
    print(queue.get())
    print("main process is done.")


multiprocessing.Queue类
method:
__init__(maxsize=0)
	初始化一个队列. 参数如下:
		maxsize - 指定队列容量. 当<=0时, 不限制容量

close()
	停止向queue中放置元素

get(block=True, timeout=None)
	从队列中移除并返回一个元素. 参数列表如下:
		block - 为True时, 没有元素可获得时阻塞
		timeout - 阻塞的最长时限, 之后仍然无法获取元素时, 抛出queue.Empty异常

put(obj, block=True, timeout=None)
	将obj放入队列. 参数列表如下:
		obj - 放入队列中的对象
		block - 为True时, 队列没有位置时阻塞
		timeout - 阻塞的最长时限, 之后仍然没有位置时, 抛出queue.Full异常


例2.
from multiprocessing import Pipe, Process
import time

def send_to(con):
    con.send([1,2,"monkey"])
    time.sleep(10)
    print(con.recv())

def recv_from(con):
    print(con.recv())
    time.sleep(5)
    con.send("knock knock")

if __name__ == '__main__':
    con1, con2 = Pipe()
    p1 = Process(target=send_to, args=(con1,))
    p2 = Process(target=recv_from, args=(con2,))
    p1.start()
    p2.start()
    p1.join()
    p2.join()
    print('main process is done.')

multiprocessing.Pipe方法
Pipe(duplex=True)
	返回两个通信接口Connection对象(con1, con2). 参数如下:
		duplex - 当为True时, 信道为双工, con1/con2皆可收发信息; 当为False时, 信道为单工, con1只能收信息, con2只能发信息

multiprocessing.connection.Connection类
method:
close()
	关闭管道连接

recv()
	从另一端接收obj

send(obj)
	发送obj到另一端




线程：CPU中的最小调度单元，同一进程的不同线程，共享同一内存空间

锁：由于同一进程下的线程共享同一内存空间，同时读取内容会造成混乱，所以需要使用互斥锁防止同时读取

例1.
由于GIL的存在, 多线程只能同时存在一个正在运行的线程
from threading import Thread, Lock

balance = 0
def machine(n):
    global balance
    balance = balance + n
    balance = balance - n

def iteration(n, lock):
    for i in range(1000000):
        lock.acquire()
        try:
            machine(n)
        finally:
            lock.release()

if __name__ == '__main__':
    lock = Lock()
    t1 = Thread(target=iteration, args=(5, lock))
    t2 = Thread(target=iteration, args=(8, lock))
    t1.start()
    t2.start()
    t1.join()
    t2.join()
    print(balance)

threading模块
function:
active_count()
	当前alive线程数量

current_thread()
	返回当前运行的线程Thread对象

enumerate()
	当前alive线程Thread对象列表

class 
Thread
variable:
name
	获取或配置线程名称

daemon
	判断或设置守护线程

method：
__init__(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)
	创建一个线程实例
	group - 保留，用于将来ThreadGroup类的实现
	target - function，线程中执行的方法. 实际被run()调用
	name - str类型，线程的名称，默认为'Thread-<N>'
	args - tuple类型，传入target方法的位置参数
	kwargs - dict类型，传入target方法的关键字参数
	daemon - bool类型，设置线程是否为守护线程。默认继承自主线程，主线程非守护线程

is_alive()
	线程是否存活

join(timeout=None)
	将创建的线程阻塞主线程，阻塞指定时间或阻塞直到线程terminate
		timeout - float格式，阻塞指定时间

run()
	线程实际执行的方法
	在子类实现中必须被重写

start()
	启动线程，使run()被调用
	当多次调用同一个线程的该方法时，抛出Runtimeerror异常


class
Lock
method:
__init__()
	初始化锁

acquire(blocking=True, timeout=-1)
	请求锁，获取后置于locked状态. 参数列表如下:
		blocking - 当为True时，如果请求的锁状态为locked，进行排队等候释放锁；当为False，如果请求的锁状态为locked，直接放弃请求
		timeout - 排队等候时间，超过该时间放弃。默认为-1，代表不限制等候时间

release()
	释放锁，释放后置于unlock状态
	如果试图释放locked状态的锁，将会引发RuntimeError异常

locked()
	查看锁的状态

**并发问题：当多个线程同时读取相同变量时，会导入互相干扰，得出错误结果。
**问题解决1：确保函数使用局部变量可避免该问题
**问题解决2：使用锁，使同一时间变量只能被一个线程访问

采用多进程(多线程)的条件：计算密集型任务 or I/O密集型任务
多进程：线程切换需要消耗CPU，并且由于GIL特性无法并行执行多个线程，所以计算密集型任务适合多进程, 并且进程数量需等于核心数

多线程：I/O密集型任务由于大部分时间处于等待I/O传输(由于CPU运算速度远高于I/O速度)，所以此时切换线程会大幅增加计算机性能，且由于线程的共享空间特性，所以I/O密集型任务使用多线程

进程/线程的优缺点:
子进程奔溃不会影响父进程, 但同一进程内的线程共享同一内存空间, 所以子线程的奔溃会影响到进程内的所有线程

并发和并行的差异：
并发(concurrency)：将时间段切分为多个时间片，在时间片执行不同内容，但同一时间片只执行一个内容. 多线程和多进程都属于并发

并行(parallel)：不同任务在不同CPU上同一时间点同时执行。python只有多进程属于并行
