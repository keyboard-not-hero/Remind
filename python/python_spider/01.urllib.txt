urllib--python原生库
1.request模块(该模块使用HTTP/1.1)
导入方法：
import urllib.request
方法：
urlopen(url, data=None, timeout, context=None)：返回http.client.HTTPResponse类。url可以为字符串或者Request对象；data为请求内容体，使用urllib.parse.urlencode进行字符串拼接并进行encode编码的bytes格式，提供后为post请求，默认为get；timeout为等待响应时间；context为ssl.SSLContext对象
build_opener(handler)：创建并返回一个urllib.request.OpenerDirector。handler为BaseHandler及其子类
getproxies()：获取proxy列表

类：
urllib.request.Request
方法：
get_method()：获取请求方法
__init__(url, data=None, headers={}, method=None)：构造请求元素。url支持字符串；data为请求内容体，使用urllib.parse.urlencode进行字符串拼接并进行encode编码的bytes格式；headers支持dict；method支持大写字符串(upper-case)，没提供data时默认为GET，提供data时默认为POST

类：
urllib.request.ProxyHandler
方法：
__init__(proxies=None)：创建一个ProxyHandler对象。proxies为dict格式，key为应用层协议，value为完整URL。proxies默认读取环境变量<protocol>_proxy(大小写均可)；当变量REQUEST_METHOD存在时，忽略<protocol>_proxy(大写)，并且可以在header中添加"proxy":URL实现；如果找不到，再从Windows注册表查找(MacOS从系统配置查找)

相关类：
urllib.request.HTTPCookieProcessor
相关方法：
__init__(cookiejar=None)：创建一个HTTPCookieProcessor对象。cookiejar参数为http.cookiejar.CookieJar对象

类：
urllib.request.OpenerDirector
方法：
open(url, data=None[, timeout])：返回http.client.HTTPResponse类。url可以为字符串或者Request对象；data为请求内容体，使用urllib.parse.urlencode进行字符串拼接并进行encode编码的bytes格式，提供后为post请求，默认为get；timeout为等待响应时间

补充类：
http.client.HTTPResponse
属性/方法：
msg：状态码描述
status：状态码
reason：状态码描述
version：HTTP版本
read()：获取已编码的响应主体，即bytes对象
getheader(name)：获取对应名称的头部
getheaders()：获取全部首部
info()：可读性更高的全部首部
geturl()：获取重定向后的URL

2.parse模块
导入方法：
import urllib.parse
方法：
urllib.parse.urlencode()：对dict或元组的序列进行组合字符串，返回str对象

问题及参数：
1.登录：data
2.浏览器限制：headers中的user-agent，默认为Python-urllib/3.6
3.防盗链(只能从自己内部站点跳转到当前界面)：headers中的referer属性
4.保存登录：CookieJarHandler
5.突破访问频次限制：ProxyHandler

示例1：简单获取网页内容，GET
request = urllib.request.Request(url = “www.dataforthompson.cn”)
response = urllib.request.urlopen(request)
html = response.read().decode(“utf-8”)
print(html)

示例2：伪装chrome并使用账号密码登录，POST
headers = {
    “user-agent”:”Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36”,
    “host”:”www.dataforthompson.cn”
}
data = {}
data[“user”] = “thompson”
data[“[password”] = “cctvcontrol”
data = urllib.parse.urlencode(data).encode(‘utf-8’)
request = urllib.request.Request(“www.dataforthompson.cn”,data=data,headers=headers)
response = urllib.request.urlopen(request)
html = response.read().decode(‘utf-8’)
print(html)

示例3：获取服务器返回的cookies(发送cookie使用header发送，cookie的所有item包含在一个字符串中)
cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open(“www.dataforthompson.cn”)
for item in cookie:
    print(item.name+”=”+item.value)

示例4：配置proxy
proxies = {
  “http”:”http://PROXY_IP:PORT”,
  “https”:”https://PROXY_IP:PORT”
}
proxy = urllib.request.ProxyHandler(proxies)
opener = urllib.request.build_opener(proxy)
response = opener.open(“http://httpbin.org/get”)
print(response.read().decode(“utf-8”))
